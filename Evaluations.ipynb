{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7865c321-ea63-4609-ae28-efb5c86ca586",
   "metadata": {},
   "source": [
    "# Evaluating Expert vs LLM-Generated Content\n",
    "\n",
    "As the two creators of our respective topics, we decided to test an AI's ability (Opus) to distinguish between expert-created and LLM-generated content for both the French language and makeup. As the AI only had a 50% success rate, only guessing the makeup slides correct, the evaluation provided some interesting insights into the nuances of this type of content evaluation.\n",
    "\n",
    "## French Language Slideshows\n",
    "\n",
    "For the French language slideshows, the AI initially guessed that the presentation with polished language and high-level cultural overview (Emma) was created by an expert, while the one focusing on basic grammar and verb conjugations was LLM-generated (Lucas). \n",
    "\n",
    "However, this turned out to be incorrect. I informed the AI that I (Lucas) was actually the creator of the grammar-focused slideshow, the one with simple verb conjugations, and that by providing concrete examples of language usage, I was the clear one with the subject matter expertise. The AI acknowledged this correction and the importance of looking beyond surface-level characteristics. My belief is that maybe the AI saw the facts on the French language as an indicator of knowledge, even though that was stuff anyone could've found on the internet. \n",
    "\n",
    "## Makeup Slideshows\n",
    "\n",
    "On the second attempt with the makeup slideshows, the AI aimed to apply the lessons learned from the French example. It correctly identified that the presentation offering detailed product breakdowns, step-by-step instructions, and knowledge of specific trends was more likely created by a makeup expert.\n",
    "\n",
    "Meanwhile, the more generic \"art of makeup\" overview was accurately pegged as a probable LLM creation. This showed an improvement in the AI's content evaluation approach.\n",
    "\n",
    "## Lessons Learned\n",
    "\n",
    "Testing the AI across these two topics highlighted the importance of looking for depth, concrete examples, and up-to-date knowledge as indicators of human expertise, rather than just relying on surface-level nonsense.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb8ff225-513d-40f5-8743-3a9b3b720a33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3.11 (Max - Klaviyo)",
   "language": "python",
   "name": "comm4190_max"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
